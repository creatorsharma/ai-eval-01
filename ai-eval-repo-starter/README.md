# AI Evaluation Project (Starter)

This repo helps you run and share small AI model evaluation studies.

## What's inside
- `prompts/` – the exact prompts you test
- `responses/<model>/` – raw model outputs
- `evaluations/` – your rubric, scores, and findings
- `scripts/` – small helper scripts
- `notebooks/` – optional analysis notebooks

## Quick start
1. Add your prompts to `prompts/` (one file per prompt).
2. Paste each model's output in `responses/<model>/` using the same filename as the prompt.
3. Score the outputs using `evaluations/rubric.md` and log scores in `evaluations/scored_results.csv`.
4. Summarize insights in `evaluations/analysis.md`.
